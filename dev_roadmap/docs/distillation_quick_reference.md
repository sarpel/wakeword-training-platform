# üöÄ Knowledge Distillation - Quick Reference

## ‚ö° Quick Start (30 seconds)

### Enable in YAML Config
```yaml
distillation:
  enabled: true
  teacher_architecture: "wav2vec2"
  temperature: 3.0
  alpha: 0.6

data:
  use_precomputed_features_for_training: false
  fallback_to_audio: true
```

### Enable in Python
```python
config = WakewordConfig()
config.distillation.enabled = True
config.distillation.temperature = 3.0
config.distillation.alpha = 0.6
config.data.use_precomputed_features_for_training = False
```

### Train
```python
trainer = DistillationTrainer(model, train_loader, val_loader, config, checkpoint_mgr, "cuda")
trainer.train()
```

---

## üìä Parameter Cheat Sheet

| Parameter | Type | Range | Default | Description |
|-----------|------|-------|---------|-------------|
| `enabled` | bool | - | `False` | Enable distillation |
| `teacher_architecture` | str | `"wav2vec2"` | `"wav2vec2"` | Teacher model type |
| `teacher_model_path` | str | Path or `""` | `""` | Custom checkpoint (empty=pretrained) |
| `temperature` | float | 1.0-10.0 | `2.0` | Softness of distributions |
| `alpha` | float | 0.0-1.0 | `0.5` | Teacher weight (0=none, 1=full) |

### Temperature Guide
- **1.0**: No softening (hard labels)
- **2.0-3.0**: Moderate (recommended)
- **4.0-6.0**: Very soft (max knowledge transfer)
- **>6.0**: Too soft (unstable)

### Alpha Guide
| Scenario | Recommended Alpha | Reason |
|----------|------------------|--------|
| Small dataset (<1k samples) | 0.6-0.8 | Rely on teacher |
| Large dataset (>10k samples) | 0.3-0.5 | Rely on data |
| Strong teacher (>90% acc) | 0.6-0.8 | Teacher is reliable |
| Weak teacher (<80% acc) | 0.2-0.4 | Don't trust teacher |
| Tiny student model | 0.7-0.9 | Need teacher guidance |

---

## üîß Common Configurations

### Configuration 1: MobileNetV3 for Mobile (Recommended)
```yaml
distillation:
  enabled: true
  temperature: 3.0
  alpha: 0.6

model:
  architecture: "mobilenetv3"
  num_classes: 2
  dropout: 0.25

training:
  batch_size: 32
  epochs: 80
```

### Configuration 2: TinyConv for ESP32 (Ultra-Light)
```yaml
distillation:
  enabled: true
  temperature: 4.0
  alpha: 0.7  # Higher for tiny model

model:
  architecture: "tiny_conv"
  num_classes: 2
  dropout: 0.2

training:
  batch_size: 32
  epochs: 150
```

### Configuration 3: ResNet18 for Accuracy
```yaml
distillation:
  enabled: true
  temperature: 2.5
  alpha: 0.5

model:
  architecture: "resnet18"
  num_classes: 2
  dropout: 0.3

training:
  batch_size: 48
  epochs: 100
```

---

## ‚ö†Ô∏è Critical Requirements

### ‚úÖ Must Have
- [ ] `distillation.enabled = True`
- [ ] `data.use_precomputed_features_for_training = False`
- [ ] `data.fallback_to_audio = True`
- [ ] `dataset.return_raw_audio = True`
- [ ] GPU with 8GB+ VRAM
- [ ] PyTorch with CUDA
- [ ] `transformers` library installed

### ‚ùå Don't Do
- ‚ùå Use precomputed spectrograms (teacher needs raw audio!)
- ‚ùå Set alpha=0.0 (no distillation happening)
- ‚ùå Set alpha=1.0 (ignoring ground truth)
- ‚ùå Use temperature > 10 (numerical instability)
- ‚ùå Train on CPU (too slow)

---

## üêõ Troubleshooting One-Liners

| Problem | Solution |
|---------|----------|
| "Teacher NOT called" | Set `return_raw_audio=True` in dataset |
| CUDA OOM | Reduce `batch_size` to 16 or 8 |
| Loss is NaN | Lower `temperature` to 2.0 |
| No improvement | Lower `alpha` to 0.3 or check teacher accuracy |
| Import error | `pip install transformers` |
| Student accuracy = 50% | Check teacher accuracy first |
| "unexpected keyword is_hard_negative" | Update `compute_loss()` signature (see full guide) |

---

## üìà Monitoring Checklist

During training, verify:
- [ ] Loss decreasing (not NaN)
- [ ] Teacher initialized: "Loading teacher model: wav2vec2"
- [ ] Teacher frozen: "Teacher model initialized and frozen"
- [ ] Both losses logged: student_loss + distillation_loss
- [ ] Validation F1 improving

---

## üß™ Quick Ablation Test

```python
# Baseline
config.distillation.enabled = False
baseline_f1 = train(config)

# Distillation
config.distillation.enabled = True
config.distillation.alpha = 0.6
distill_f1 = train(config)

improvement = distill_f1 - baseline_f1
print(f"Distillation gain: +{improvement:.2%}")
```

Expected gain: **+2% to +5% F1**

---

## üéØ Typical Results

| Student Model | Baseline F1 | With Distillation | Gain |
|---------------|-------------|-------------------|------|
| MobileNetV3 | 82.5% | 86.3% | +3.8% |
| TinyConv | 75.2% | 79.8% | +4.6% |
| ResNet18 | 88.1% | 90.4% | +2.3% |

*Results on typical wakeword dataset (~5k samples)*

---

## üìù Code Templates

### Template 1: Basic Usage
```python
from src.config.defaults import WakewordConfig, DistillationConfig
from src.training.distillation_trainer import DistillationTrainer

config = WakewordConfig()
config.distillation = DistillationConfig(enabled=True, alpha=0.6, temperature=3.0)
config.data.use_precomputed_features_for_training = False

trainer = DistillationTrainer(model, train_loader, val_loader, config, checkpoint_mgr, "cuda")
trainer.train()
```

### Template 2: Load from YAML
```python
config = WakewordConfig.load("config/distillation.yaml")
trainer = DistillationTrainer(model, train_loader, val_loader, config, checkpoint_mgr, "cuda")
trainer.train()
```

### Template 3: Custom Teacher Checkpoint
```python
config.distillation.teacher_model_path = "checkpoints/my_teacher.pt"
trainer = DistillationTrainer(...)
```

---

## üîç Quick Diagnostics

### Check if distillation is active:
```python
print(f"Distillation enabled: {config.distillation.enabled}")
print(f"Alpha: {config.distillation.alpha}")
print(f"Temperature: {config.distillation.temperature}")
```

### Verify teacher loaded:
```python
# In training logs, look for:
# "Loading teacher model: wav2vec2"
# "Teacher model initialized and frozen"
```

### Check raw audio pipeline:
```python
batch = next(iter(train_loader))
print(f"Input shape: {batch['input'].shape}")
# Should be (batch, samples) for raw audio
# NOT (batch, 1, freq, time) for spectrograms
```

---

## üí° Tips and Tricks

1. **Start conservative**: `alpha=0.5, temp=2.0`
2. **Increase alpha** if student struggles (small model/dataset)
3. **Increase temp** for more knowledge transfer
4. **Monitor teacher accuracy** - if <80%, may not help
5. **Use mixed precision** to save VRAM
6. **Log both losses** separately to debug

---

## üìö File Locations

| File | Purpose |
|------|---------|
| `src/training/distillation_trainer.py` | Main implementation |
| `src/models/huggingface.py` | Teacher (Wav2Vec2) |
| `src/config/defaults.py:220` | DistillationConfig |
| `tests/test_distillation_trainer.py` | Tests |
| `docs/knowledge_distillation_guide.md` | Full guide |

---

## üéì Key Concepts (One-Liners)

- **Teacher**: Large, accurate model (Wav2Vec2, ~95M params)
- **Student**: Small, efficient model (MobileNetV3, ~1.5M params)
- **Temperature**: Softens probability distributions (default: 2.0)
- **Alpha**: Balance between teacher and ground truth (default: 0.5)
- **KL Divergence**: Measures difference between student and teacher
- **Soft Labels**: Probability distributions instead of hard 0/1 labels

---

## ‚è±Ô∏è Time Estimates

| Model | Dataset Size | Training Time (RTX 3060) |
|-------|--------------|--------------------------|
| MobileNetV3 | 5k samples | ~2 hours (80 epochs) |
| TinyConv | 5k samples | ~3 hours (150 epochs) |
| ResNet18 | 5k samples | ~4 hours (100 epochs) |

*With distillation, add ~30% overhead due to teacher forward pass*

---

## üöÄ Next Steps

1. Read full guide: `docs/knowledge_distillation_guide.md`
2. Try example config: Enable in UI (Panel 2 ‚Üí Optimization ‚Üí Distillation)
3. Run ablation: Compare with/without distillation
4. Tune hyperparameters: Adjust alpha and temperature
5. Export student: Use ONNX for deployment

---

**Need Help?** Check the full guide in `docs/knowledge_distillation_guide.md`
