{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ OpenWakeWord Training Platform - A100 Optimized\n",
        "\n",
        "**Complete training pipeline for high-accuracy wake word detection models.**\n",
        "\n",
        "## Features\n",
        "- A100 GPU optimized (batch_size=256, torch.compile, BF16)\n",
        "- x86_64 target (ResNet18, 80 Mel bands, 2.0s audio)\n",
        "- Full augmentation pipeline (noise, RIR, SpecAugment)\n",
        "- EMA, mixed precision, early stopping\n",
        "- ONNX export for deployment\n",
        "\n",
        "## Dataset\n",
        "Expects dataset at: `/content/drive/My Drive/OpenWakeWord_Backups/dataset.tar.gz`\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ GPU Check & System Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi\n",
        "\n",
        "# Install system dependencies\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq libsndfile1 ffmpeg\n",
        "\n",
        "import torch\n",
        "print(f\"\\n‚úÖ PyTorch: {torch.__version__}\")\n",
        "print(f\"‚úÖ CUDA: {torch.version.cuda}\")\n",
        "print(f\"‚úÖ Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "# Detect A100 for optimizations\n",
        "IS_A100 = 'A100' in torch.cuda.get_device_name(0) if torch.cuda.is_available() else False\n",
        "print(f\"‚úÖ A100 Detected: {IS_A100}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ Install Python Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install required packages (suppress output)\n",
        "!pip install librosa==0.10.0.post2 soundfile==0.12.1 resampy==0.4.2\n",
        "!pip install scikit-learn==1.3.0 tqdm pyyaml structlog\n",
        "!pip install onnx onnxruntime-gpu\n",
        "!pip install matplotlib seaborn plotly\n",
        "\n",
        "print(\"‚úÖ All packages installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ Mount Google Drive & Extract Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import tarfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Dataset paths\n",
        "DATASET_TAR = Path('/content/drive/My Drive/OpenWakeWord_Backups/dataset.tar.gz')\n",
        "DATASET_DIR = Path('/content/dataset')\n",
        "OUTPUT_DIR = Path('/content/output')\n",
        "\n",
        "# Create directories\n",
        "DATASET_DIR.mkdir(exist_ok=True)\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Extract dataset\n",
        "if DATASET_TAR.exists():\n",
        "    print(f\"üì¶ Extracting {DATASET_TAR.name}...\")\n",
        "    with tarfile.open(DATASET_TAR, 'r:gz') as tar:\n",
        "        tar.extractall(DATASET_DIR)\n",
        "    print(f\"‚úÖ Extracted to {DATASET_DIR}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Dataset not found: {DATASET_TAR}\")\n",
        "\n",
        "# List contents\n",
        "print(\"\\nüìÇ Dataset structure:\")\n",
        "for item in sorted(DATASET_DIR.iterdir()):\n",
        "    if item.is_dir():\n",
        "        count = len(list(item.rglob('*.wav'))) + len(list(item.rglob('*.mp3')))\n",
        "        print(f\"  {item.name}/: {count} audio files\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ Configuration (A100 Optimized, x86_64 Target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import List, Optional, Dict, Any\n",
        "import json\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"A100-optimized configuration for x86_64 deployment\"\"\"\n",
        "    # Paths\n",
        "    data_root: str = \"/content/dataset\"\n",
        "    output_dir: str = \"/content/output\"\n",
        "    \n",
        "    # Audio parameters\n",
        "    sample_rate: int = 16000\n",
        "    audio_duration: float = 2.0  # 2s for x86_64 (more context)\n",
        "    n_mels: int = 80  # High resolution for x86_64\n",
        "    n_fft: int = 512\n",
        "    hop_length: int = 160\n",
        "    \n",
        "    # Model\n",
        "    architecture: str = \"resnet18\"\n",
        "    num_classes: int = 2\n",
        "    dropout: float = 0.4\n",
        "    \n",
        "    # Training (A100 optimized)\n",
        "    batch_size: int = 256  # Large batch for A100\n",
        "    epochs: int = 100\n",
        "    learning_rate: float = 0.001\n",
        "    weight_decay: float = 0.02\n",
        "    num_workers: int = 4\n",
        "    early_stopping_patience: int = 20\n",
        "    \n",
        "    # Optimizer\n",
        "    optimizer: str = \"adamw\"\n",
        "    scheduler: str = \"cosine\"\n",
        "    warmup_epochs: int = 5\n",
        "    gradient_clip: float = 1.0\n",
        "    mixed_precision: bool = True\n",
        "    \n",
        "    # Loss\n",
        "    loss_function: str = \"focal_loss\"\n",
        "    focal_alpha: float = 0.75\n",
        "    focal_gamma: float = 2.0\n",
        "    label_smoothing: float = 0.1\n",
        "    \n",
        "    # Augmentation\n",
        "    time_stretch_range: tuple = (0.85, 1.15)\n",
        "    pitch_shift_range: tuple = (-3, 3)\n",
        "    noise_prob: float = 0.6\n",
        "    noise_snr_range: tuple = (3.0, 20.0)\n",
        "    rir_prob: float = 0.5\n",
        "    spec_augment: bool = True\n",
        "    freq_mask_param: int = 20\n",
        "    time_mask_param: int = 40\n",
        "    \n",
        "    # EMA\n",
        "    use_ema: bool = True\n",
        "    ema_decay: float = 0.999\n",
        "    \n",
        "    # Checkpointing\n",
        "    checkpoint_dir: str = \"/content/output/checkpoints\"\n",
        "    save_to_drive: bool = True\n",
        "    drive_checkpoint_dir: str = \"/content/drive/My Drive/OpenWakeWord_Backups/checkpoints\"\n",
        "\n",
        "# Create config\n",
        "config = Config()\n",
        "\n",
        "# Adjust for non-A100 GPUs\n",
        "if not IS_A100:\n",
        "    config.batch_size = 64\n",
        "    print(\"‚ö†Ô∏è Non-A100 GPU detected, reduced batch_size to 64\")\n",
        "\n",
        "# Print config\n",
        "print(\"üìã Configuration:\")\n",
        "for key, value in asdict(config).items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ Core Modules (Inline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = False\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "set_seed(42)\n",
        "print(\"‚úÖ Core imports ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6Ô∏è‚É£ Audio Processing & Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AudioProcessor:\n",
        "    \"\"\"Audio loading and preprocessing\"\"\"\n",
        "    def __init__(self, sample_rate=16000, duration=2.0):\n",
        "        self.sample_rate = sample_rate\n",
        "        self.duration = duration\n",
        "        self.target_length = int(sample_rate * duration)\n",
        "    \n",
        "    def load_audio(self, path):\n",
        "        \"\"\"Load and preprocess audio file\"\"\"\n",
        "        try:\n",
        "            audio, sr = librosa.load(path, sr=self.sample_rate, mono=True)\n",
        "            # Pad or trim to target length\n",
        "            if len(audio) < self.target_length:\n",
        "                audio = np.pad(audio, (0, self.target_length - len(audio)))\n",
        "            else:\n",
        "                audio = audio[:self.target_length]\n",
        "            return audio.astype(np.float32)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {path}: {e}\")\n",
        "            return np.zeros(self.target_length, dtype=np.float32)\n",
        "\n",
        "class MelSpectrogramExtractor(nn.Module):\n",
        "    \"\"\"GPU-accelerated mel spectrogram extraction\"\"\"\n",
        "    def __init__(self, sample_rate=16000, n_mels=80, n_fft=512, hop_length=160):\n",
        "        super().__init__()\n",
        "        self.mel_spec = T.MelSpectrogram(\n",
        "            sample_rate=sample_rate,\n",
        "            n_fft=n_fft,\n",
        "            hop_length=hop_length,\n",
        "            n_mels=n_mels,\n",
        "            power=2.0\n",
        "        )\n",
        "        self.amplitude_to_db = T.AmplitudeToDB(stype='power', top_db=80)\n",
        "    \n",
        "    def forward(self, waveform):\n",
        "        # waveform: (B, 1, samples) or (B, samples)\n",
        "        if waveform.dim() == 2:\n",
        "            waveform = waveform.unsqueeze(1)\n",
        "        mel = self.mel_spec(waveform.squeeze(1))\n",
        "        mel_db = self.amplitude_to_db(mel)\n",
        "        return mel_db.unsqueeze(1)  # (B, 1, n_mels, time)\n",
        "\n",
        "print(\"‚úÖ Audio processors ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7Ô∏è‚É£ Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AudioAugmentation(nn.Module):\n",
        "    \"\"\"Audio augmentation pipeline\"\"\"\n",
        "    def __init__(self, sample_rate=16000, noise_files=None, rir_files=None,\n",
        "                 noise_prob=0.5, rir_prob=0.25, snr_range=(5, 20)):\n",
        "        super().__init__()\n",
        "        self.sample_rate = sample_rate\n",
        "        self.noise_prob = noise_prob\n",
        "        self.rir_prob = rir_prob\n",
        "        self.snr_range = snr_range\n",
        "        \n",
        "        # Load noise files\n",
        "        self.noises = []\n",
        "        if noise_files:\n",
        "            for f in noise_files[:200]:  # Limit for memory\n",
        "                try:\n",
        "                    audio, sr = librosa.load(f, sr=sample_rate, mono=True)\n",
        "                    if len(audio) > sample_rate:  # At least 1s\n",
        "                        self.noises.append(torch.from_numpy(audio).float())\n",
        "                except:\n",
        "                    pass\n",
        "        print(f\"  Loaded {len(self.noises)} noise files\")\n",
        "        \n",
        "        # Load RIR files\n",
        "        self.rirs = []\n",
        "        if rir_files:\n",
        "            for f in rir_files[:100]:  # Limit for memory\n",
        "                try:\n",
        "                    audio, sr = librosa.load(f, sr=sample_rate, mono=True)\n",
        "                    rir = torch.from_numpy(audio).float()\n",
        "                    rir = rir / (rir.abs().max() + 1e-8)\n",
        "                    self.rirs.append(rir)\n",
        "                except:\n",
        "                    pass\n",
        "        print(f\"  Loaded {len(self.rirs)} RIR files\")\n",
        "    \n",
        "    def add_noise(self, waveform):\n",
        "        if not self.noises or random.random() > self.noise_prob:\n",
        "            return waveform\n",
        "        \n",
        "        noise = random.choice(self.noises)\n",
        "        target_len = waveform.shape[-1]\n",
        "        \n",
        "        # Crop or loop noise\n",
        "        if len(noise) > target_len:\n",
        "            start = random.randint(0, len(noise) - target_len)\n",
        "            noise = noise[start:start + target_len]\n",
        "        else:\n",
        "            noise = noise.repeat((target_len // len(noise)) + 1)[:target_len]\n",
        "        \n",
        "        noise = noise.to(waveform.device)\n",
        "        \n",
        "        # Calculate SNR\n",
        "        snr = random.uniform(*self.snr_range)\n",
        "        signal_power = waveform.pow(2).mean()\n",
        "        noise_power = noise.pow(2).mean()\n",
        "        scale = torch.sqrt(signal_power / (noise_power * (10 ** (snr / 10)) + 1e-8))\n",
        "        \n",
        "        return waveform + scale * noise\n",
        "    \n",
        "    def apply_rir(self, waveform):\n",
        "        if not self.rirs or random.random() > self.rir_prob:\n",
        "            return waveform\n",
        "        \n",
        "        rir = random.choice(self.rirs).to(waveform.device)\n",
        "        original_len = waveform.shape[-1]\n",
        "        \n",
        "        # Convolve\n",
        "        waveform_padded = F.pad(waveform, (0, len(rir) - 1))\n",
        "        rir_flipped = rir.flip(0).unsqueeze(0).unsqueeze(0)\n",
        "        waveform_2d = waveform_padded.unsqueeze(0).unsqueeze(0)\n",
        "        convolved = F.conv1d(waveform_2d, rir_flipped).squeeze()\n",
        "        \n",
        "        # Trim and normalize\n",
        "        convolved = convolved[:original_len]\n",
        "        convolved = convolved / (convolved.abs().max() + 1e-8)\n",
        "        \n",
        "        # Dry/wet mix\n",
        "        wet_ratio = random.uniform(0.3, 0.7)\n",
        "        return waveform * wet_ratio + convolved * (1 - wet_ratio)\n",
        "    \n",
        "    def forward(self, waveform):\n",
        "        waveform = self.add_noise(waveform)\n",
        "        waveform = self.apply_rir(waveform)\n",
        "        return waveform\n",
        "\n",
        "class SpecAugment(nn.Module):\n",
        "    \"\"\"SpecAugment for spectrograms\"\"\"\n",
        "    def __init__(self, freq_mask=20, time_mask=40, n_freq=2, n_time=2):\n",
        "        super().__init__()\n",
        "        self.freq_mask = T.FrequencyMasking(freq_mask)\n",
        "        self.time_mask = T.TimeMasking(time_mask)\n",
        "        self.n_freq = n_freq\n",
        "        self.n_time = n_time\n",
        "    \n",
        "    def forward(self, spec):\n",
        "        for _ in range(self.n_freq):\n",
        "            spec = self.freq_mask(spec)\n",
        "        for _ in range(self.n_time):\n",
        "            spec = self.time_mask(spec)\n",
        "        return spec\n",
        "\n",
        "print(\"‚úÖ Augmentation modules ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8Ô∏è‚É£ Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WakewordDataset(Dataset):\n",
        "    \"\"\"Wake word detection dataset\"\"\"\n",
        "    def __init__(self, files, labels, audio_processor, augmentation=None):\n",
        "        self.files = files\n",
        "        self.labels = labels\n",
        "        self.audio_processor = audio_processor\n",
        "        self.augmentation = augmentation\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        audio = self.audio_processor.load_audio(self.files[idx])\n",
        "        audio_tensor = torch.from_numpy(audio).float()\n",
        "        \n",
        "        # Apply augmentation if in training mode\n",
        "        if self.augmentation is not None:\n",
        "            audio_tensor = self.augmentation(audio_tensor)\n",
        "        \n",
        "        return audio_tensor, self.labels[idx]\n",
        "\n",
        "def scan_dataset(data_root):\n",
        "    \"\"\"Scan dataset folder and return file lists\"\"\"\n",
        "    data_root = Path(data_root)\n",
        "    files = []\n",
        "    labels = []\n",
        "    \n",
        "    # Positive samples (label = 1)\n",
        "    pos_dir = data_root / 'positive'\n",
        "    if pos_dir.exists():\n",
        "        pos_files = list(pos_dir.rglob('*.wav')) + list(pos_dir.rglob('*.mp3'))\n",
        "        files.extend(pos_files)\n",
        "        labels.extend([1] * len(pos_files))\n",
        "        print(f\"  Positive: {len(pos_files)} files\")\n",
        "    \n",
        "    # Negative samples (label = 0)\n",
        "    neg_dir = data_root / 'negative'\n",
        "    if neg_dir.exists():\n",
        "        neg_files = list(neg_dir.rglob('*.wav')) + list(neg_dir.rglob('*.mp3'))\n",
        "        files.extend(neg_files)\n",
        "        labels.extend([0] * len(neg_files))\n",
        "        print(f\"  Negative: {len(neg_files)} files\")\n",
        "    \n",
        "    # Background noise (for augmentation)\n",
        "    bg_dir = data_root / 'background'\n",
        "    bg_files = list(bg_dir.rglob('*.wav')) if bg_dir.exists() else []\n",
        "    print(f\"  Background noise: {len(bg_files)} files\")\n",
        "    \n",
        "    # RIRs (for augmentation)\n",
        "    rir_dir = data_root / 'rirs'\n",
        "    rir_files = list(rir_dir.rglob('*.wav')) + list(rir_dir.rglob('*.flac')) if rir_dir.exists() else []\n",
        "    print(f\"  RIRs: {len(rir_files)} files\")\n",
        "    \n",
        "    return [str(f) for f in files], labels, bg_files, rir_files\n",
        "\n",
        "# Scan dataset\n",
        "print(\"üìÇ Scanning dataset...\")\n",
        "all_files, all_labels, bg_files, rir_files = scan_dataset(config.data_root)\n",
        "print(f\"\\n‚úÖ Total: {len(all_files)} audio files\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9Ô∏è‚É£ Train/Val/Test Split & DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split dataset\n",
        "train_files, temp_files, train_labels, temp_labels = train_test_split(\n",
        "    all_files, all_labels, test_size=0.3, random_state=42, stratify=all_labels\n",
        ")\n",
        "val_files, test_files, val_labels, test_labels = train_test_split(\n",
        "    temp_files, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
        ")\n",
        "\n",
        "print(f\"üìä Dataset splits:\")\n",
        "print(f\"  Train: {len(train_files)} (pos: {sum(train_labels)}, neg: {len(train_labels)-sum(train_labels)})\")\n",
        "print(f\"  Val:   {len(val_files)} (pos: {sum(val_labels)}, neg: {len(val_labels)-sum(val_labels)})\")\n",
        "print(f\"  Test:  {len(test_files)} (pos: {sum(test_labels)}, neg: {len(test_labels)-sum(test_labels)})\")\n",
        "\n",
        "# Create processors\n",
        "audio_processor = AudioProcessor(config.sample_rate, config.audio_duration)\n",
        "\n",
        "# Create augmentation (for training only)\n",
        "print(\"\\nüîä Loading augmentation files...\")\n",
        "train_augmentation = AudioAugmentation(\n",
        "    sample_rate=config.sample_rate,\n",
        "    noise_files=bg_files,\n",
        "    rir_files=rir_files,\n",
        "    noise_prob=config.noise_prob,\n",
        "    rir_prob=config.rir_prob,\n",
        "    snr_range=config.noise_snr_range\n",
        ")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = WakewordDataset(train_files, train_labels, audio_processor, train_augmentation)\n",
        "val_dataset = WakewordDataset(val_files, val_labels, audio_processor, None)\n",
        "test_dataset = WakewordDataset(test_files, test_labels, audio_processor, None)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, \n",
        "                          num_workers=config.num_workers, pin_memory=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False,\n",
        "                        num_workers=config.num_workers, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False,\n",
        "                         num_workers=config.num_workers, pin_memory=True)\n",
        "\n",
        "print(f\"\\n‚úÖ DataLoaders ready: {len(train_loader)} train batches, {len(val_loader)} val batches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîü Model Architecture (ResNet18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "class ResNet18Wakeword(nn.Module):\n",
        "    \"\"\"ResNet18 for wake word detection\"\"\"\n",
        "    def __init__(self, num_classes=2, dropout=0.4):\n",
        "        super().__init__()\n",
        "        self.resnet = models.resnet18(weights=None)\n",
        "        # Modify first conv for single channel input\n",
        "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        # Replace classifier\n",
        "        num_features = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(num_features, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# Create model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = ResNet18Wakeword(num_classes=config.num_classes, dropout=config.dropout)\n",
        "model = model.to(device)\n",
        "\n",
        "# Use channels_last for better performance\n",
        "model = model.to(memory_format=torch.channels_last)\n",
        "\n",
        "# Compile for A100 (PyTorch 2.0+)\n",
        "if IS_A100 and hasattr(torch, 'compile'):\n",
        "    model = torch.compile(model, mode='max-autotune')\n",
        "    print(\"‚úÖ torch.compile enabled (max-autotune)\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nüß† Model: ResNet18\")\n",
        "print(f\"  Total params: {total_params:,}\")\n",
        "print(f\"  Trainable: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£1Ô∏è‚É£ Loss Function & Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"Focal Loss for imbalanced classification\"\"\"\n",
        "    def __init__(self, alpha=0.75, gamma=2.0, label_smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.label_smoothing = label_smoothing\n",
        "    \n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', \n",
        "                                  label_smoothing=self.label_smoothing)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        \n",
        "        # Apply alpha weighting\n",
        "        alpha_t = torch.where(targets == 1, self.alpha, 1 - self.alpha)\n",
        "        focal_loss = alpha_t * (1 - pt) ** self.gamma * ce_loss\n",
        "        \n",
        "        return focal_loss.mean()\n",
        "\n",
        "# Create loss, optimizer, scheduler\n",
        "criterion = FocalLoss(alpha=config.focal_alpha, gamma=config.focal_gamma, \n",
        "                      label_smoothing=config.label_smoothing)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, \n",
        "                              weight_decay=config.weight_decay)\n",
        "\n",
        "# Cosine scheduler with warmup\n",
        "total_steps = len(train_loader) * config.epochs\n",
        "warmup_steps = len(train_loader) * config.warmup_epochs\n",
        "\n",
        "def lr_lambda(step):\n",
        "    if step < warmup_steps:\n",
        "        return step / warmup_steps\n",
        "    progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
        "    return 0.5 * (1 + np.cos(np.pi * progress))\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# Mixed precision scaler\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=config.mixed_precision)\n",
        "\n",
        "# Feature extractor & SpecAugment\n",
        "mel_extractor = MelSpectrogramExtractor(\n",
        "    sample_rate=config.sample_rate, n_mels=config.n_mels,\n",
        "    n_fft=config.n_fft, hop_length=config.hop_length\n",
        ").to(device)\n",
        "\n",
        "spec_augment = SpecAugment(\n",
        "    freq_mask=config.freq_mask_param, time_mask=config.time_mask_param\n",
        ").to(device) if config.spec_augment else None\n",
        "\n",
        "print(\"‚úÖ Loss, optimizer, scheduler ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£2Ô∏è‚É£ EMA (Exponential Moving Average)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EMA:\n",
        "    \"\"\"Exponential Moving Average for model weights\"\"\"\n",
        "    def __init__(self, model, decay=0.999):\n",
        "        self.model = model\n",
        "        self.decay = decay\n",
        "        self.shadow = {}\n",
        "        self.backup = {}\n",
        "        \n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name] = param.data.clone()\n",
        "    \n",
        "    def update(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name] = self.decay * self.shadow[name] + (1 - self.decay) * param.data\n",
        "    \n",
        "    def apply_shadow(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.backup[name] = param.data.clone()\n",
        "                param.data = self.shadow[name]\n",
        "    \n",
        "    def restore(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                param.data = self.backup[name]\n",
        "        self.backup = {}\n",
        "\n",
        "ema = EMA(model, decay=config.ema_decay) if config.use_ema else None\n",
        "print(f\"‚úÖ EMA: {'enabled' if ema else 'disabled'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£3Ô∏è‚É£ Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "import time\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, scheduler, scaler, mel_extractor, spec_augment, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    pbar = tqdm(loader, desc='Training', leave=False)\n",
        "    for audio, labels in pbar:\n",
        "        audio = audio.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        \n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        \n",
        "        with torch.cuda.amp.autocast(enabled=config.mixed_precision):\n",
        "            # Extract features\n",
        "            features = mel_extractor(audio)\n",
        "            features = features.to(memory_format=torch.channels_last)\n",
        "            \n",
        "            # Apply SpecAugment\n",
        "            if spec_augment is not None and model.training:\n",
        "                features = spec_augment(features)\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "        \n",
        "        # Update EMA\n",
        "        if ema:\n",
        "            ema.update()\n",
        "        \n",
        "        # Metrics\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        \n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100*correct/total:.1f}%'})\n",
        "    \n",
        "    return total_loss / len(loader), correct / total\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, criterion, mel_extractor, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    \n",
        "    for audio, labels in tqdm(loader, desc='Validating', leave=False):\n",
        "        audio = audio.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        \n",
        "        with torch.cuda.amp.autocast(enabled=config.mixed_precision):\n",
        "            features = mel_extractor(audio)\n",
        "            features = features.to(memory_format=torch.channels_last)\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        probs = F.softmax(outputs, dim=1)\n",
        "        _, predicted = outputs.max(1)\n",
        "        \n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_probs.extend(probs[:, 1].cpu().numpy())\n",
        "    \n",
        "    # Calculate metrics\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_probs = np.array(all_probs)\n",
        "    \n",
        "    acc = (all_preds == all_labels).mean()\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    \n",
        "    # FPR, FNR\n",
        "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
        "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "    \n",
        "    return {\n",
        "        'loss': total_loss / len(loader),\n",
        "        'acc': acc, 'f1': f1, 'precision': precision, 'recall': recall,\n",
        "        'fpr': fpr, 'fnr': fnr, 'cm': cm, 'probs': all_probs, 'labels': all_labels\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Training functions ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£4Ô∏è‚É£ Training Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training state\n",
        "best_f1 = 0\n",
        "best_epoch = 0\n",
        "epochs_without_improvement = 0\n",
        "history = {'train_loss': [], 'val_loss': [], 'val_f1': [], 'val_fpr': [], 'val_fnr': []}\n",
        "\n",
        "# Checkpointing\n",
        "checkpoint_dir = Path(config.checkpoint_dir)\n",
        "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"üöÄ Starting training for {config.epochs} epochs...\")\n",
        "print(f\"   Batch size: {config.batch_size}, LR: {config.learning_rate}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(config.epochs):\n",
        "    epoch_start = time.time()\n",
        "    \n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(\n",
        "        model, train_loader, criterion, optimizer, scheduler, \n",
        "        scaler, mel_extractor, spec_augment, device\n",
        "    )\n",
        "    \n",
        "    # Validate (use EMA weights if available)\n",
        "    if ema:\n",
        "        ema.apply_shadow()\n",
        "    \n",
        "    val_metrics = validate(model, val_loader, criterion, mel_extractor, device)\n",
        "    \n",
        "    if ema:\n",
        "        ema.restore()\n",
        "    \n",
        "    # Update history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_metrics['loss'])\n",
        "    history['val_f1'].append(val_metrics['f1'])\n",
        "    history['val_fpr'].append(val_metrics['fpr'])\n",
        "    history['val_fnr'].append(val_metrics['fnr'])\n",
        "    \n",
        "    # Check improvement\n",
        "    if val_metrics['f1'] > best_f1:\n",
        "        best_f1 = val_metrics['f1']\n",
        "        best_epoch = epoch\n",
        "        epochs_without_improvement = 0\n",
        "        \n",
        "        # Save best model\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict() if not hasattr(model, '_orig_mod') else model._orig_mod.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_metrics': val_metrics,\n",
        "            'config': asdict(config)\n",
        "        }\n",
        "        torch.save(checkpoint, checkpoint_dir / 'best_model.pt')\n",
        "        improved = \"‚úÖ NEW BEST\"\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        improved = \"\"\n",
        "    \n",
        "    epoch_time = time.time() - epoch_start\n",
        "    lr = scheduler.get_last_lr()[0]\n",
        "    \n",
        "    print(f\"Epoch {epoch+1:3d}/{config.epochs} | \"\n",
        "          f\"Train Loss: {train_loss:.4f} | \"\n",
        "          f\"Val Loss: {val_metrics['loss']:.4f} | \"\n",
        "          f\"F1: {val_metrics['f1']:.4f} | \"\n",
        "          f\"FPR: {val_metrics['fpr']:.4f} | \"\n",
        "          f\"FNR: {val_metrics['fnr']:.4f} | \"\n",
        "          f\"LR: {lr:.6f} | \"\n",
        "          f\"{epoch_time:.1f}s {improved}\")\n",
        "    \n",
        "    # Early stopping\n",
        "    if epochs_without_improvement >= config.early_stopping_patience:\n",
        "        print(f\"\\n‚èπÔ∏è Early stopping at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(\"=\" * 60)\n",
        "print(f\"‚úÖ Training complete in {total_time/3600:.2f} hours\")\n",
        "print(f\"üèÜ Best F1: {best_f1:.4f} at epoch {best_epoch+1}\")\n",
        "\n",
        "# Copy best model to Drive\n",
        "if config.save_to_drive:\n",
        "    drive_dir = Path(config.drive_checkpoint_dir)\n",
        "    drive_dir.mkdir(parents=True, exist_ok=True)\n",
        "    import shutil\n",
        "    shutil.copy(checkpoint_dir / 'best_model.pt', drive_dir / 'best_model.pt')\n",
        "    print(f\"üíæ Model saved to Drive: {drive_dir / 'best_model.pt'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£5Ô∏è‚É£ Evaluation on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model\n",
        "checkpoint = torch.load(checkpoint_dir / 'best_model.pt', map_location=device)\n",
        "if hasattr(model, '_orig_mod'):\n",
        "    model._orig_mod.load_state_dict(checkpoint['model_state_dict'])\n",
        "else:\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"üìä Evaluating on test set...\")\n",
        "test_metrics = validate(model, test_loader, criterion, mel_extractor, device)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìà TEST SET RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Accuracy:  {test_metrics['acc']*100:.2f}%\")\n",
        "print(f\"  F1 Score:  {test_metrics['f1']:.4f}\")\n",
        "print(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
        "print(f\"  Recall:    {test_metrics['recall']:.4f}\")\n",
        "print(f\"  FPR:       {test_metrics['fpr']:.4f}\")\n",
        "print(f\"  FNR:       {test_metrics['fnr']:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(f\"  TN={test_metrics['cm'][0,0]:5d}  FP={test_metrics['cm'][0,1]:5d}\")\n",
        "print(f\"  FN={test_metrics['cm'][1,0]:5d}  TP={test_metrics['cm'][1,1]:5d}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£6Ô∏è‚É£ Training Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Loss curves\n",
        "axes[0, 0].plot(history['train_loss'], label='Train')\n",
        "axes[0, 0].plot(history['val_loss'], label='Validation')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].set_title('Training & Validation Loss')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True)\n",
        "\n",
        "# F1 Score\n",
        "axes[0, 1].plot(history['val_f1'], color='green')\n",
        "axes[0, 1].axhline(y=best_f1, color='r', linestyle='--', label=f'Best: {best_f1:.4f}')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('F1 Score')\n",
        "axes[0, 1].set_title('Validation F1 Score')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True)\n",
        "\n",
        "# ROC Curve\n",
        "fpr_curve, tpr_curve, _ = roc_curve(test_metrics['labels'], test_metrics['probs'])\n",
        "roc_auc = auc(fpr_curve, tpr_curve)\n",
        "axes[1, 0].plot(fpr_curve, tpr_curve, label=f'ROC (AUC = {roc_auc:.4f})')\n",
        "axes[1, 0].plot([0, 1], [0, 1], 'k--')\n",
        "axes[1, 0].set_xlabel('False Positive Rate')\n",
        "axes[1, 0].set_ylabel('True Positive Rate')\n",
        "axes[1, 0].set_title('ROC Curve')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True)\n",
        "\n",
        "# FPR/FNR over epochs\n",
        "axes[1, 1].plot(history['val_fpr'], label='FPR', color='red')\n",
        "axes[1, 1].plot(history['val_fnr'], label='FNR', color='blue')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Rate')\n",
        "axes[1, 1].set_title('FPR & FNR Over Training')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUTPUT_DIR / 'training_results.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f\"üìä Results saved to {OUTPUT_DIR / 'training_results.png'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£7Ô∏è‚É£ Export to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import onnx\n",
        "\n",
        "# Prepare model for export\n",
        "if hasattr(model, '_orig_mod'):\n",
        "    export_model = model._orig_mod\n",
        "else:\n",
        "    export_model = model\n",
        "\n",
        "export_model.eval()\n",
        "export_model = export_model.to('cpu')\n",
        "\n",
        "# Create dummy input\n",
        "n_samples = int(config.sample_rate * config.audio_duration)\n",
        "n_frames = n_samples // config.hop_length + 1\n",
        "dummy_input = torch.randn(1, 1, config.n_mels, n_frames)\n",
        "\n",
        "# Export\n",
        "onnx_path = OUTPUT_DIR / 'wakeword_model.onnx'\n",
        "torch.onnx.export(\n",
        "    export_model,\n",
        "    dummy_input,\n",
        "    str(onnx_path),\n",
        "    opset_version=14,\n",
        "    input_names=['input'],\n",
        "    output_names=['output'],\n",
        "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
        ")\n",
        "\n",
        "# Validate\n",
        "onnx_model = onnx.load(str(onnx_path))\n",
        "onnx.checker.check_model(onnx_model)\n",
        "\n",
        "# Size\n",
        "size_mb = onnx_path.stat().st_size / (1024 * 1024)\n",
        "\n",
        "print(f\"‚úÖ ONNX model exported: {onnx_path}\")\n",
        "print(f\"üì¶ Model size: {size_mb:.2f} MB\")\n",
        "\n",
        "# Copy to Drive\n",
        "if config.save_to_drive:\n",
        "    drive_dir = Path(config.drive_checkpoint_dir)\n",
        "    shutil.copy(onnx_path, drive_dir / 'wakeword_model.onnx')\n",
        "    print(f\"üíæ ONNX saved to Drive: {drive_dir / 'wakeword_model.onnx'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Training Complete!\n",
        "\n",
        "**Outputs saved to:**\n",
        "- Best checkpoint: `/content/output/checkpoints/best_model.pt`\n",
        "- ONNX model: `/content/output/wakeword_model.onnx`\n",
        "- Training plots: `/content/output/training_results.png`\n",
        "\n",
        "**Copied to Google Drive:**\n",
        "- `/content/drive/My Drive/OpenWakeWord_Backups/checkpoints/`"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}